; Sample worker node parameters
[sample]
parameter=Sample Parameter
iterations=10
sleep_sec=2

; General purpose worker node parameters
[server]

; Maximum number of jobs(threads) can be served simultaneously.
; This parameter defines job parallelism. For computationally intensive
; algorithms this value should not be more than number of CPUs
max_threads=4

; initial number of threads created for incoming jobs
init_threads=2

; TCP/IP and UDP port number for control messages (like shutdown, version)
; and job notifications. It runs special control thread for incoming 
; administrative requests (from netschedule_control and netschedule_admin)
control_port=9300

; Server side logging. A worker node can ask its context if this flag is 
; set to true
log=true

; Logging system type. Could be self_rotating, rotating, non_rotating
; Default is sefl_rotating.
; log_type = rotating


; Log file size(in bytes). When the size of the log file has reached given
; size and log_type parameter is not set to non_rotating it will be rotated. 
; Default is 1Mb
; log_file_size = 1000000


; Internal. 
; Delay in seconds node task dispatcher waits for free
; space in the job queue. Lower value usually gives better response
; to shutdown command (CPU traded off)
thread_pool_timeout=5

; Time worker node spends waiting for new jobs without connecting to
; the netschedule server queue. Server sends UPD requests to wake the 
; node up. Bigger values of this parameter reduces the netschedule server
; load in expense of job delivery latency (because of 
; potential loss of UDP packages)
;
job_wait_timeout=10

; The max total number of jobs after which the node will shutdown itself.
; Restarting the node periodically is useful due to accumulating heap 
; fragmentation possible leaks etc.
; default is 0 - means unlimited number of jobs.
; max_total_jobs = 100

; When true, server transforms into a daemon, detached from the 
; current program group (for UNIX only)
;daemon=true

; The list of worker nodes which this node will check before attempting 
; to retrieve a job from the NetSchedule queue. If at least one of these 
; worker nodes has an ideal thread, this node will not connect to the queue
; for a job. This node and all nodes from the given list must be connected 
; to the same NetSchedule service, the same queue and must run the same job. 
; If the list is empty (defult) then this node is a master.
;master_nodes = service3:9300, service2:9300

; List of network hosts which are allowed admin access to the worker node
; if this worker node is controled by grid_node_watcher.sh don't forget to
; to add "localhost" to this list. By default any host is allowed.
;admin_hosts = localhost service1 service2 service3

; Time delay (in seconds) between the node enters an idle mode (all jobs are done
; and there are no new jobs in the queue) and the idle task gets executed.
; Can not be less then 1 sec. Default value is 30 sec.
;idle_run_delay = 30

; Specifies if an idle task works in an exclusive mode, which means that no real job 
; will be processed until the idle task is running. Default value is true.
;idle_exclusive = true

[gw_debug]
; Prefix for all debug files
run_name = debug_run

; Whether to gather requests or to execute them
;mode = gather

; Whether to gather requests or to execute them
; default is 1.
; Input  blobs will be dumped  to "<RUNNAME>.<pid>.[1..N].inp"
; Output blobs will be dumped  to "<RUNNAME>.<pid>.[1..N].out"
; Messages     will be dumped  to "<RUNNAME>.<pid>.[1..N].msg"
; Diagnostics  will be written to "<RUNNAME>.<pid>.log"
;gather_nrequests = 2

; Whether to gather requests or to execute them
;mode = execute

; Process requests from the following files
; [no default]
; Input  blobs will be read  from "<fileN>"
; Output blobs will be dumped  to "<fileN>__<pid>.execute.out"
; Messages     will be dumped  to "<fileN>__<pid>.execute.msg"
; Diagnostics  will be written to "<RUNNAME>__<pid>.execute.log"
;execute_requests = <file1> [<file2> ...]
 


; NetSchedule client configuration
; 
[netschedule_client]
; use LB (load balanced service) or pre-defined server (non LB configuration)
.include=connection_ns_lb
client_name=node_sample

; Name of the queue (DO NOT use default queue for your application)
queue_name=sample


; Network storage (NetCache) configuration
;
[netcache_client]
.include = connection_nc_lb
client_name = node_sample

; If this parameter is set to true then before the job is processed the input data blob 
; will be stored into a file on the local file system and the job will get a stream to 
; that file. 
cache_input = false

; If this parameter is set to true then the job will get a stream to file on the local 
; file system as an output stream. After job is done the content of this file will be
; stored to a blob in the storage.
cache_output = true


[connection_ns_lb]
; LB service name
service=NS_test
; Time interval in seconds when client is automatically rebanaced
rebalance_time=10
; Number of requests before rebalancing
rebalance_requests=100
; Specifies a list of netschedule sevices available for this connection
; very useful for debugging
;sevices_list=didimo:9100


; NetSchedule connection parameter used when LB is not available
;
[connection_ns]
host=didimo
port=9100



[connection_nc_lb]
; LB service name
service=NC_test
; Time interval in seconds when client is automatically rebanaced
rebalance_time=10
; Number of requests before rebalancing
rebalance_requests=100
; I/O volume based rebalancing
rebalance_bytes=5000000

; NetCache connection parameter used when LB is not available
;
[connection_nc]
host=didimo
port=9000


